---
title: "The Bootstrap"
description: |
  A description of the Bootstrap, and an example from "All of Statistics"
date: "2020-07-24"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)

theme_set(theme_minimal())
```


Suppose we have a (small) dataset, and a particular statistic $T$ (ie quantity computable from data) we are interested in.
The Bias/Variance tradeoff tells us that if our statistic $T$ is an unbiased estimator of some quantity $\theta$, then the mean square error $\mathbb{E}[(\theta-T)^2]$ is completely determined by the variance of our statistic.
So understanding the variance is crucial to understanding how well our statistic is performing.

For simple enough statistics, we can determine the variance directly -- but for even slightly more complicated cases, the underlying mathematics quickly gets too difficult to handle gracefully.

This is where the *Bootstrap* enters the stage.
The Bootstrap is a way to estimate the distribution *of the statistic $T$* from the dataset itself, and then use that distribution to estimate new information about $T$ -- such as its variance, or forming confidence intervals for $T$ on a specific dataset, etc.

Basically, the Bootstrap approximates the true distribution on the data with the *empirical distribution* given by the data: in the empirical distribution, each data point has an equal probability $1/N$ of appearing.
Sampling from this empirical distribution amounts to sampling (with replacement!!) from the data itself.

## Example

For a concrete example, Wasserman (in All of Statistics, Example 8.6) picks up a dataset used by Bradley Efron to illustrate the Bootstrap:

```{r}
lsat.gpa = tibble(
  lsat = c(
    576, 635, 558, 578, 666, 580, 555, 661,
    651, 605, 653, 575, 545, 572, 594
  ),
  gpa  = c(
    3.39, 3.30, 2.81, 3.03, 3.44, 3.07, 3.00, 3.43,
    3.36, 3.13, 3.12, 2.74, 2.76, 2.88, 3.96
  )
)
ggplot(lsat.gpa, aes(lsat, gpa)) +
  geom_point()
```

The task is to estimate the correlation between LSAT scores and GPA scores from this set of `r NROW(lsat.gpa)` observations.
Whereas Wasserman looked at sample correlation, we can go one step further -- let's find distributions for a linear model: predicting GPA using LSAT.
Our model would be
\[
GPA \sim \beta_1 LSAT + \beta_0
\]
and easily calculated using `lm(gpa~lsat, lsat.gpa)`.
Computing this generates the model parameters as:

```{r}
lsat.gpa.lm = lm(gpa~lsat, lsat.gpa)
lsat.gpa.lm$coefficients %>% kable()
```

And we can plot the resulting fitted model with the data.

```{r}
ggplot(lsat.gpa, aes(lsat, gpa)) +
  geom_point() +
  geom_abline(slope = lsat.gpa.lm$coefficients["lsat"],
              intercept = lsat.gpa.lm$coefficients["(Intercept)"])
```

But this is just the expected values for intercept and slope for the model.
We do not yet know how stable these values are -- how large their respective variances come out to be.
This is where the Bootstrap steps in to help.

## The Bootstrap Method

The Bootstrap proceeds as follows:

1. Pick bootstrap size $B$.
2. $B$ times do:
   - Sample $N$ values $X^*_b$ with replacement from your data $X$
   - Calculate $T^*_b = T(X^*_b)$
3. Collect all the $T*^_b$: their distribution approximates the sampling distribution of $T$ on the original data.

So let's get ourselves bootstrap estimates of the slope and intercept.

```{r}
B = 1000
lsat.gpa.lm.boot = sapply(1:B, function(b) {
  lsat.gpa.star = lsat.gpa %>% sample_frac(replace = TRUE)
  lm(gpa~lsat, lsat.gpa.star)$coefficients
}) %>% t %>% as_tibble %>% rename(intercept=`(Intercept)`)
```

We can see the distribution of slope/intercept pairs in a heatmap as follows:

```{r}
ggplot(lsat.gpa.lm.boot,aes(intercept, lsat)) +
  geom_hex()
```

And we can see its effects on the sampled lines as follows:

```{r}
ggplot(lsat.gpa, aes(lsat, gpa)) +
  geom_abline(data=lsat.gpa.lm.boot,
              aes(intercept=intercept, slope=lsat),
              alpha=0.025) +
  geom_point()
```

The distributions of the slope and intercept from the Bootstrap sample are:

```{r}
ggplot(lsat.gpa.lm.boot %>% pivot_longer(everything()),
       aes(value)) +
  geom_density() +
  facet_grid(rows=vars(name), scales="free")
```

This plot wasn't entirely helpful. We can get a better view at the sampling distribution for the slope by plotting it in isolation:

```{r}
ggplot(lsat.gpa.lm.boot, aes(lsat)) +
  geom_density()
```

### Confidence intervals

Wasserman suggests three approaches for confidence intervals:

1. Use a normal approximation. If the Bootstrap sample data follows approximately a normal distribution, then we can use $T(X) \pm z_{\alpha/2}\widehat{se}$ as a confidence interval, where $\widehat{se}$ is the standard deviation of the Bootstrap sample.
2. Use *Pivotal Intervals*. Suppose we could write $H$ for the CDF of quantity $T-\theta$. Then a confidence interval for $\theta$ could be given by $(T-H^{-1}(1-\alpha/2), T-H^{-1}(\alpha/2))$. Now, since $\theta$ is unknown, so is $H$ -- but we can approximate $H$ using our Bootstrap! Use $\hat{H}(r) = 1/B \sum_b(\# T^*_b-T(X) \leq r)$. Then by taking quantiles in the set of $r^*_b = T^*_b-T(X)$ we get a confidence interval as $(T(X)-r^*_{(1-\alpha/2)}, T(X)-r^*_{(\alpha/2)})$ (here, I write $r^*_{(q)}$ for the $q$th quantile of the $r^*$). This reduces to the confidence interval $(T(X) - T^*_{(1-\alpha/2)}, T(X)-T^*_{(\alpha/2)})$.
3. Just take quantiles directly from the $T^*$ quantities to form a *bootstrap percentile interval*.

So to compute the 95% two-sided confidence intervals, we could use the following R-code:

```{r}
boot.ci.normal = c(
  lo=lsat.gpa.lm$coefficients["(Intercept)"]+
    qnorm(0.025)*sd(lsat.gpa.lm.boot$intercept), 
  hi=lsat.gpa.lm$coefficients["(Intercept)"]+
    qnorm(0.975)*sd(lsat.gpa.lm.boot$intercept),
  lo=lsat.gpa.lm$coefficients["lsat"]+
    qnorm(0.025)*sd(lsat.gpa.lm.boot$lsat),
  hi=lsat.gpa.lm$coefficients["lsat"]+
    qnorm(0.975)*sd(lsat.gpa.lm.boot$lsat)
)

boot.ci.pivotal = c(
  lo=2*lsat.gpa.lm$coefficients["(Intercept)"]-
    quantile(lsat.gpa.lm.boot$intercept, 0.975), 
  hi=2*lsat.gpa.lm$coefficients["(Intercept)"]-
    quantile(lsat.gpa.lm.boot$intercept, 0.025), 
  lo=2*lsat.gpa.lm$coefficients["lsat"]-
    quantile(lsat.gpa.lm.boot$lsat,0.975),
  hi=2*lsat.gpa.lm$coefficients["lsat"]-
    quantile(lsat.gpa.lm.boot$lsat,0.025)
)

boot.ci.percentage = c(
  `lo.(Interval)`=quantile(lsat.gpa.lm.boot$intercept, 0.025),
  `hi.(Interval)`=quantile(lsat.gpa.lm.boot$intercept, 0.975),
  lo.lsat=quantile(lsat.gpa.lm.boot$lsat, 0.025),
  hi.lsat=quantile(lsat.gpa.lm.boot$lsat, 0.975)
)

rbind(
  boot.ci.normal, boot.ci.pivotal, boot.ci.percentage
) %>% kable()
```



